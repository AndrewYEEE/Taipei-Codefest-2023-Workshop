# -*- coding: utf-8 -*-
"""hackathon_workshop_ETL.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1k0PfE-9HxbOdWy4qhccVEGWabvITPHDE

# Data Preprocessing Introduction

本節課將示範一些常用的基本資料處理操作與檢查，並且分享TUIC內部使用的處理function，旨在讓大家能執行基本的資料處理。環境將使用Colab線上執行python，並主要透過pandas、geopandas兩個packages操作。上半堂聚焦於一般型態資料處理，下半堂聚焦於空間資料處理。  

*(本競賽不限制資料處理方式，python是現今流行的資料科學程式語言，但並非唯一的工具，當資料很小又不需要自動化時，Excel也是不錯的工具。)*

資料前處理的本質是為了將資料處理成下個階段可以使用的形式，要做什麼處理完全取決於原始資料與目標間的差別。因此在正式開始資料處理前，建議可參考以下步驟:

1. Understanding Input and Output Data
2. Designing Processing Plans
3. Coding

Coding內容完全視資料與目標而定，並無固定方式，以下提供我們常用的結構，請隨你的需求調整:
"""

# 0.Set Config
# 1.Collection
# 2.Inspection
# 3.Rename
# 4.Define data type
# 5.Fill missing value
# 6....... other process ......
# 7.Select
# 8.Save

"""# 實作 - 原始資料初步清理轉換

## Install necessary packages
"""

!pip install geocoder==1.38.1
!pip install geopandas==0.13.2
!pip install shapely==2.0.2
!pip install keplergl==0.3.2

"""import and pre-load funtion"""

# Commented out IPython magic to ensure Python compatibility.
# 上傳./src/func.py，並執行以下程序

import geopandas as gpd
import geocoder
import json
import pandas as pd
from shapely.geometry import LineString, Point, Polygon

from google.colab import files
uploaded = files.upload()
# %run func.py

"""本節課將實作2個資料流，並提供1個資料流作為課堂練習。  
實作2個資料流如下，其中B資料流在上半堂不會完成，其中涉及的空間處理將在本節的下半堂繼續。  
(*格式: [input data]+[input data] -> [output data]*)
* A.臺北市身心障礙者福利服務 -> 無障礙需求歷年趨勢
* B.身障機構收容暨空位狀態+身障就業資源地圖+村里界 -> 身障友善機構

## **A.臺北市身心障礙者福利服務 -> 無障礙需求歷年趨勢**

### Designing Processing Plan

(實際步驟是先了解`Input data`與`Output data`才制定出`處理計畫`，但為了閱讀方便，將計畫放在最前面)
1. 修改欄位名
2. 民國年修改成西元年
3. 定義data type
4. 選擇指定欄位
"""

# Ouput data- 無障礙需求歷年趨勢
RAW_DATA_URL = 'https://raw.githubusercontent.com/tpe-doit/Taipei-Codefest-2023-Workshop/2-Initial-Data-Cleaning-Visualization/Datasets/Processed/%E7%84%A1%E9%9A%9C%E7%A4%99%E9%9C%80%E6%B1%82%E6%AD%B7%E5%B9%B4%E8%B6%A8%E5%8B%A2.csv?token=GHSAT0AAAAAACHYWWNICD23CYPEE25UQQSSZJXOERA'
FILE_NAME = 'accessibility_need_over_the_years.csv'
!wget $RAW_DATA_URL -O $FILE_NAME
accessibility_need = pd.read_csv(FILE_NAME)

accessibility_need.head()

# Input data- 臺北市身心障礙者福利服務
disability_welfare = get_datataipei_api('a09d5ff1-6b83-4bce-abbb-480b126db611')
disability_welfare.shape

disability_welfare.head(3)

disability_welfare.dtypes

"""1. 修改欄位名
2. 民國年修改成西元年
3. 定義data type
4. 選擇指定欄位

### ETL code

預設儲存資料於`/content`路徑下
"""

import pandas as pd

# Designing Processing Plan
# 1. 修改欄位名，[]變成()
# 2. 民國年修改成西元年
# 3. 定義data type，具量詞欄位應為float

# 0.Set Config
RID = 'a09d5ff1-6b83-4bce-abbb-480b126db611'
GITHUB_DATA_URL = 'https://raw.githubusercontent.com/tpe-doit/Taipei-Codefest-2023-Workshop/2-Initial-Data-Cleaning-Visualization/Datasets/Processed/%E7%84%A1%E9%9A%9C%E7%A4%99%E9%9C%80%E6%B1%82%E6%AD%B7%E5%B9%B4%E8%B6%A8%E5%8B%A2.csv?token=GHSAT0AAAAAACHYWWNICD23CYPEE25UQQSSZJXOERA'
GITHUB_DOWNLOAD_FILE_NAME = 'accessibility_need_over_the_years.csv'
OUTPUT_FILENAME = 'disability_welfare_over_the_years.csv'

# 1.Collection
# input 臺北市身心障礙者福利服務
raw_disability_welfare = get_datataipei_api(RID)
# output sample 無障礙需求歷年趨勢
!wget $GITHUB_DATA_URL -O $GITHUB_DOWNLOAD_FILE_NAME
raw_accessibility_need = pd.read_csv(GITHUB_DOWNLOAD_FILE_NAME)

disability_welfare = raw_disability_welfare.copy()
accessibility_need = raw_accessibility_need.copy()

# 2.Inspection
print('臺北市身心障礙者福利服務:')
print(disability_welfare.columns)

print('無障礙需求歷年趨勢:')
print(accessibility_need.columns)

disability_welfare.isna().sum(axis=0)

disability_welfare['住宿及照顧福利機構核定服務人數[人]'].value_counts(dropna=False)

# 3.Rename
# 修改欄位名，[]變成()
# disability_welfare.rename(columns={'old_column_name': 'new_column_name'}, inplace=True)
# disability_welfare.columns = ['column_name1', 'column_name2']

# loop
col_map = {}
for col in disability_welfare.columns:
    new_col = col.replace('[', '(').replace(']', ')')
    col_map[col] = new_col
disability_welfare.rename(columns=col_map, inplace=True)
disability_welfare.rename(columns={'年別': '年份'}, inplace=True)
disability_welfare.columns

# 4.Define data type
# 定義data type，具量詞欄位應為float
# (int無法容許np.nan)
disability_welfare['年份'] = disability_welfare['年份'].astype(str)
df_row_len, df_col_len = disability_welfare.shape
for col_index in range(3, df_col_len):
    col_name = disability_welfare.columns[col_index]
    fine_col = disability_welfare[col_name].copy()
    is_dash = (fine_col=='-')
    fine_col.loc[is_dash] = None
    fine_col = fine_col.astype(float)
    disability_welfare[col_name] = fine_col
disability_welfare.dtypes

# 5.Fill missing value
disability_welfare.isna().sum(axis=0)

disability_welfare.iloc[:, 3:].describe()

# 6. ......other process......
# 民國年修改成西元年
chinese_year = disability_welfare['年份'].str.replace('年', '')
bc_year = chinese_year.astype(int) + 1911
bc_year = bc_year.astype(str) + '年'
disability_welfare['年份'] = bc_year
disability_welfare['年份']

# 7.Select
# 年份                 object
# 日間及住宿式照顧補助金額(元)    object
# 生活補助金額(元)          object
# 輔具補助金額(元)          object
select_col = ['年份', '日間及住宿式照顧補助金額(元)', '生活補助金額(元)', '輔具補助金額(元)']
disability_welfare = disability_welfare[select_col]
disability_welfare.head()

accessibility_need.head()

# 8.Save
disability_welfare.to_csv(OUTPUT_FILENAME, index=False, encoding='UTF-8')

"""可下載處理完的資料`./content/disability_welfare_over_the_years.csv`

## **B.臺北市身障設施+臺北市身障社區長照機構+村里界 -> 身障友善機構**

### Designing Processing Plan

將`身障機構收容暨空位狀態`與`臺北市身障社區長照機構`合併成一個資料集，並產生適合的空間資訊。
1. 一致的columns name
2. 正確的column type
3. `臺北市身障社區長照機構`須加上"屬性"
4. 利用地址為`臺北市身障社區長照機構`加上行政區
5. (下半)利用Geocoder取得座標
6. (下半)產生TWD97座標值
7. (下半)利用村里界取得行政區
8. (下半)合併`身障機構收容暨空位狀態`與`臺北市身障社區長照機構`
"""

# Output data- 身障友善機構
URL = 'https://raw.githubusercontent.com/tpe-doit/Taipei-Codefest-2023-Workshop/3-ETL/Datasets/Processed/%E8%BA%AB%E9%9A%9C%E5%8F%8B%E5%96%84%E6%A9%9F%E6%A7%8B.csv?token=GHSAT0AAAAAACHYWWNJ7QSB7BYLARDE6UTCZJYZ54A'
FILE_NAME = 'disable_friendly_institutions.csv'
!wget $URL -O $FILE_NAME

# 上傳data
friendly_institution = pd.read_csv(FILE_NAME)

friendly_institution.head()

# 身障設施
URL = 'https://data.taipei/api/frontstage/tpeod/dataset/resource.download?rid=57d8db7f-21a9-413f-8158-1df3b3d66d75'
FILE_NAME = 'disable_institutions.zip'
!wget $URL -O $FILE_NAME
!unzip -d disable_institutions -O BIG5 $FILE_NAME

disable_institution = gpd.read_file('./disable_institutions/身障設施_202309.shp')

disable_institution.head(3)

# 臺北市身障社區長照機構
# https://data.taipei/dataset/detail?id=06a3d0c3-1949-4501-b16f-a0c4c036e2d5

# 直接儲存到RAM
rid = '5e6dd32d-ac91-48cd-87ab-776fcc4811b7'
raw_disability_community_longterm_care_institutions = get_datataipei_api(rid)

# 上傳再讀取
# upload file to Colab
# raw_disability_community_longterm_care_institutions = pd.read_csv(
#     '身障社區長照機構.csv', encoding='big5'
# )
dis_longterm = raw_disability_community_longterm_care_institutions.copy()
print(dis_longterm)

"""### ETL Code

將`身障機構收容暨空位狀態`與`臺北市身障社區長照機構`合併成一個資料集，並產生適合的空間資訊。
1. 一致的columns name
2. 正確的column type
3. `臺北市身障社區長照機構`須加上"屬性"
4. 利用地址為`臺北市身障社區長照機構`加上行政區
5. (下半)利用Geocoder取得座標
6. (下半)產生TWD97座標值
7. (下半)利用村里界取得行政區
8. (下半)合併`身障機構收容暨空位狀態`與`臺北市身障社區長照機構`
"""

# 0.Set Config
INSTITUTION_URL = 'https://data.taipei/api/frontstage/tpeod/dataset/resource.download?rid=57d8db7f-21a9-413f-8158-1df3b3d66d75'
INSTITUTION_FILE_NAME = 'disable_institutions.zip'
INSTITUTION_FOLDER = 'disable_institutions'
LONGTERM_RID = '5e6dd32d-ac91-48cd-87ab-776fcc4811b7'
GITHUB_DATA_URL = 'https://raw.githubusercontent.com/tpe-doit/Taipei-Codefest-2023-Workshop/3-ETL/Datasets/Processed/%E8%BA%AB%E9%9A%9C%E5%8F%8B%E5%96%84%E6%A9%9F%E6%A7%8B.csv?token=GHSAT0AAAAAACHYWWNIZWPL36D3VIXR74BUZJY233Q'
GITHUB_DOWNLOAD_FILE_NAME = 'disable_friendly_institution.csv'
OUTPUT_FILENAME = 'disable_friendly_facility.csv'

# 1.Collection
# Input- 身障機構收容暨空位狀態 ???
# 臺北市身障設施
!wget $URL -O $FILE_NAME
!unzip -d $INSTITUTION_FOLDER -O BIG5 -o $INSTITUTION_FILE_NAME
raw_disable_institution = gpd.read_file(
    f'./{INSTITUTION_FOLDER}/身障設施_202309.shp', from_crs='EPSG:3826'
)
# Input- 臺北市身障社區長照機構
raw_disability_community_longterm_care_institutions = get_datataipei_api(LONGTERM_RID)
# output sample 身障友善機構
!wget $GITHUB_DATA_URL -O $GITHUB_DOWNLOAD_FILE_NAME
raw_friendly_institution = pd.read_csv(GITHUB_DOWNLOAD_FILE_NAME)

# copy raw data
# 臺北市身障社區長照機構
dis_longterm = raw_disability_community_longterm_care_institutions.copy()
# 臺北市身障設施 (shp)
dis_institution = raw_disable_institution.copy()
# Output
friendly_institution = raw_friendly_institution.copy()

# 1.5 remove the institutions in New Taipei City
dis_institution = dis_institution[
    ~dis_institution.address.str.startswith('新北市')]

# 2.Inspection
dis_institution.head(3)

dis_longterm.head()

dis_institution['type'].value_counts()

dis_longterm['機構類型'].value_counts()

# 3.Rename
col_map = {
    '機構類型': 'type',
    '機構名稱': 'name',
    '地址': 'addr',
    '電話': 'tel'
}
dis_longterm.rename(columns=col_map, inplace=True)

dis_institution.rename(columns={'lon': 'lng'}, inplace=True)

# 4.Define data type
# 5.Fill missing value
# 6....... other process ......
# 3. `臺北市身障社區長照機構`須加上"屬性"
dis_longterm['attr'] = '待查'
# 4. 利用地址為`臺北市身障社區長照機構`加上行政區
# 正則表達式取得指定字串，'臺北市([非數字的]{2個字}區)'
dis_longterm['town'] = dis_longterm['addr'].str.extract('臺北市([^\\d]{2}區)')

dis_longterm

"""### 正則表達式:
https://docs.python.org/zh-tw/3/howto/regex.html
"""

import re

pattern = '123'  # 比對給定文字
text = "123 abc 456123"
result = re.findall(pattern, text)
print(result)

pattern = '^123'  # 左邊界接給定文字
result = re.findall(pattern, text)
print(result)

pattern = '456(123)'  # group
result = re.findall(pattern, text)
print(result)

pattern = '456\(123\)'  # group
result = re.findall(pattern, text)
print(result)

pattern = '456\(123\)'  # group
result = re.findall(pattern, '123 456(123)')
print(result)

pattern = '\\d{6}'  # 6個連續的數字
result = re.findall(pattern, text)
print(result)

pattern = '\\d{3,6}'  # 3~6個連續的數字
result = re.findall(pattern, text)
print(result)

pattern = '[ab1]'  # 給定的其中一個字元
result = re.findall(pattern, text)
print(result)

pattern = '[^ab1]'  # 非給定的其中一個字元
result = re.findall(pattern, text)
print(result)

pattern = '.*3'  # 任何數量的任意字元，後面接著3
result = re.findall(pattern, text)
print(result)

pattern = '.{,2}3'  # 最多2個任意字元，後面接著3
result = re.findall(pattern, text)
print(result)

"""# 常用資料處理function

另提供一些上面實作沒用到，但我們常用到的範例。
"""

# Collection
# read KML
gpd.io.file.fiona.drvsupport.supported_drivers['KML'] = 'rw'
df = gpd.read_file(files, driver='KML')

# Cleaning
data[col].fillna(0)
.zfill()

# Reshape
disability_welfare.melt()
disability_welfare.pivot()

set(dis_longterm['id']) - set(dis_institution['id'])
left_outer = dis_longterm.merge(dis_institution, how='left', on='id')
left_outer.loc[left_outer[]'id_y'].isna()]

# Transformation
select_col = ['年份', '日間及住宿式照顧補助金額(元)', '生活補助金額(元)', '輔具補助金額(元)']
disability_welfare = disability_welfare[select_col]
cut, qcut
time

# Aggregation
groupby(['col1', 'col2']).agg({'': 'nunique'})

"""# Break

# 空間資料處理 (geopandas)

## Data structures

GeoPandas implements two main data structures, a GeoSeries and a GeoDataFrame. These are subclasses of pandas.Series and pandas.DataFrame, respectively.

GeoPandas has three basic classes of geometric objects (which are actually shapely objects):
- Points / Multi-Points
- Lines / Multi-Lines
- Polygons / Multi-Polygons

Note that all entries in a GeoSeries need not be of the same geometric type, although certain export operations will fail if this is not the case.

### Read data
"""

# 身障社區長照機構
dis_longterm.head()

"""### Point

### address to coordinate

Due to the fact that many open datasets only contain address information and lack latitude and longitude data, obtaining coordinates is essential for preprocessing the data and performing certain tasks. There are various methods available for converting addresses into latitude and longitude coordinates. In our case, we use the "geocoder" package to accomplish this, but you are free to choose your preferred method.
"""

# 反查位置
addr_0 = dis_longterm.loc[0, 'addr'] #獲取地址 第一筆
addr_0_info = geocoder.arcgis(addr_0)
addr_0_info.json

# get lng and lat of addr_0
# 從查詢結果取出經緯度
addr_0_lng = addr_0_info.json['lng']
addr_0_lat = addr_0_info.json['lat']

# get lng and lat of addr_1
# 第二筆反查，取出經緯度
addr_1 = dis_longterm.loc[1, 'addr']
addr_1_info = geocoder.arcgis(addr_1)
addr_1_lng = addr_1_info.json['lng']
addr_1_lat = addr_1_info.json['lat']

# transfer string to Point
point0 = Point(addr_0_lng, addr_0_lat) #製作座標點0
point1 = Point(addr_1_lng, addr_1_lat) #製作座標點1

# get info of point0
print(type(point0))
print(point0.area) #製作座標點面積
print(point0.bounds) # minx, miny, maxx, maxy #製作座標點面積範圍
print(point0.geom_type)
print(point0.is_valid)

# get distance
print(point0.distance(point1)) #製作座標點0與1距離
print(point0.distance(point0))
print(point1.distance(point1))

print(((point0.x-point1.x)** 2 + (point0.y-point1.y)** 2)** (1/2))

"""### buffer"""

point0.buffer(1)

point0.buffer(1).area

point0.buffer(1, resolution=1)

point0.buffer(1, resolution=1).area

point0.buffer(1, resolution=2)

res_list = [10** x for x in range(8)]
for res in res_list:
    print(point0.buffer(1, resolution=res).area)

"""### LineString"""

first_line_string = LineString([point0, point1])

first_line_string

print(point0.x)
print(point0.y)
print('- '* 10)
print(point1.x)
print(point1.y)

# get info of point0
print(first_line_string.xy)
print(type(first_line_string))
print(first_line_string.area)
print(first_line_string.bounds) # minx, miny, maxx, maxy
print(first_line_string.geom_type)
print(first_line_string.is_valid)
print(first_line_string.length)
print(first_line_string.centroid)

"""### Polygon"""

# get lat and lng of addr_2
addr_2 = dis_longterm.loc[2, 'addr']
addr_2_info = geocoder.arcgis(addr_2)
addr_2_lng = addr_2_info.json['lng']
addr_2_lat = addr_2_info.json['lat']
point2 = Point(addr_2_lng, addr_2_lat)

first_polygon = Polygon((point0, point1, point2))
first_polygon

"""## GeoSeries"""

# define a geoseries
s = gpd.GeoSeries(
    [
        Polygon([(0, 0), (0, 1), (1, 1), (1, 0)]),
        Polygon([(10, 0), (10, 5), (0, 0)]),
        Polygon([(0, 0), (2, 2), (2, 0)]),
        LineString([(0, 0), (1, 1), (0, 1)]),
        Point(0, 1)
    ]
)

s.length

s.area

s.bounds

s.boundary

s.total_bounds

s.geom_type

s.distance(Point(-1, -1))

"""## GeoDataFrame"""

d = {
    'col1': ['name1', 'name2'],
    'geometry': [Point(1, 2), Point(2, 1)]
}
gdf = gpd.GeoDataFrame(d, crs='EPSG:4326', geometry='geometry')
gdf

"""###  crs (Coordinate Reference System)
The coordinate reference system (CRS) is important because the geometric shapes in a GeoSeries or GeoDataFrame object are simply a collection of coordinates in an arbitrary space. A CRS tells Python how those coordinates relate to places on the Earth.

The same CRS can often be referred to in many ways. For example, one of the most commonly used CRS is the WGS84 latitude-longitude projection. This can be referred to using the authority code **"EPSG:4326"**.

In Taiwan, the commonly used Coordinate Reference System (CRS) is **"EPSG:3826.**" After converting from EPSG:4326 to EPSG:3826, the units for its x and y coordinates become **meter**.
"""

# get 臺北市區界圖 from data.taipei
URL = 'https://data.taipei/api/frontstage/tpeod/dataset/resource.download?rid=d8b7eb29-136f-49fc-b14b-3489d3656122'
FILE_NAME = 'tpe_district_border.zip'
!wget $URL -O $FILE_NAME
!unzip -d tpe_district_border -O BIG5 $FILE_NAME

district_border = gpd.read_file('tpe_district_border', encoding='utf-8')
district_border.head(2)

district_border.crs = 'EPSG:3826'
district_border.crs
district_border.head(2)

district_border = district_border[['PTNAME', 'geometry']]
district_border.head(3)

district_border.crs = 'EPSG:3826'
district_border.crs

district_border = district_border.to_crs('EPSG:4326')
district_border.crs

district_border.head(3)

dis_longterm

# set column 'geometry'
# def square(x):
# 	return x ** 2
# ==lambda x: x ** 2

dis_longterm['geometry'] = dis_longterm['addr'].apply(
    lambda x: Point(
        geocoder.arcgis(x).json['lng'],
        geocoder.arcgis(x).json['lat']
    )
)

# from pd to gpd
print(dis_longterm.head(3))
dis_longterm = gpd.GeoDataFrame(
    dis_longterm,
    crs='EPSG:4326'
)
print(dis_longterm.head(3))
# change crs
dis_longterm = dis_longterm.to_crs('EPSG:3826')
print(dis_longterm.head(3))

# get distance (unit = m)
dis_longterm.loc[0, 'geometry'].distance(
    dis_longterm.loc[1, 'geometry']
)

dis_longterm

"""## Manipulations

### Intersect
E.g.: get the districts which intersect with the first long-term institution with buffer 1000
"""

# change crs
district_border = district_border.to_crs('EPSG:3826') #轉台灣常用座標格式

# get buffer
dis_longterm['buffer'] = dis_longterm['geometry'].buffer(1000) #回傳給定距離內所有點的近似表示。
print(dis_longterm.head(3))
buffer0 = dis_longterm.loc[0, 'buffer'] #獲取buffer第一筆

# get the districts that intersect with buffer0
idx = buffer0.intersects(district_border['geometry']) #以第一筆交互所有資料的座標看有無重疊
print(idx) #有11筆資料，True者重疊
print('- '* 20)
print(district_border[idx].PTNAME.tolist()) #獲取重疊的地區名稱

"""### Overlay
Perform spatial overlay between two GeoDataFrames.
"""

polys1 = gpd.GeoSeries(
    [
        Polygon([(0,0), (2,0), (2,2), (0,2)]),
        Polygon([(2,2), (4,2), (4,4), (2,4)])
    ]
)


polys2 = gpd.GeoSeries(
    [
        Polygon([(1,1), (3,1), (3,3), (1,3)]),
        Polygon([(3,3), (5,3), (5,5), (3,5)])
    ]
)


df1 = gpd.GeoDataFrame({'geometry': polys1, 'df1':[1,2]})

df2 = gpd.GeoDataFrame({'geometry': polys2, 'df2':[1,2]})

ax = df1.plot(color='red');

df2.plot(ax=ax, color='green', alpha=0.5)

# how='union'
res_union = df1.overlay(df2, how='union')
res_union

res_union.explode().reset_index()

ax = res_union.plot(alpha=0.5, cmap='tab10')
df1.plot(ax=ax, facecolor='none', edgecolor='k')
df2.plot(ax=ax, facecolor='none', edgecolor='k')

# how='intersection'
res_intersection = df1.overlay(df2, how='intersection')
res_intersection

ax = res_intersection.plot(cmap='tab10')

df1.plot(ax=ax, facecolor='none', edgecolor='k');

df2.plot(ax=ax, facecolor='none', edgecolor='k');

'''
how='symmetric_difference'
the opposite of 'intersection'
'''
res_symdiff = df1.overlay(df2, how='symmetric_difference')
ax = res_symdiff.plot(cmap='tab10')
df1.plot(ax=ax, facecolor='none', edgecolor='k');
df2.plot(ax=ax, facecolor='none', edgecolor='k');

'''
how='difference'
obtain the geometries that are part of df1 but are not contained in df2
'''
res_difference = df1.overlay(df2, how='difference')
ax = res_difference.plot(cmap='tab10')
df1.plot(ax=ax, facecolor='none', edgecolor='k');
df2.plot(ax=ax, facecolor='none', edgecolor='k');

'''
how='identity'
the result consists of the surface of df1,
but with the geometries obtained from overlaying df1 with df2
'''
res_identity = df1.overlay(df2, how='identity')
ax = res_identity.plot(cmap='tab10')
df1.plot(ax=ax, facecolor='none', edgecolor='k');
df2.plot(ax=ax, facecolor='none', edgecolor='k');

"""### E.g. the overlap area between long-term care facilities with buffer and administrative districts"""

dis_longterm_buffer = dis_longterm[['name', 'buffer']]
print(dis_longterm_buffer)
dis_longterm_buffer = dis_longterm_buffer.set_geometry('buffer')
print(dis_longterm_buffer)

overlay_df = gpd.overlay(
    dis_longterm_buffer,
    district_border,
    how='union'
).explode().reset_index()

overlay_df

overlay_df['geometry'].area

"""# Kepler in Python"""

from keplergl import KeplerGl
from google.colab import output
output.enable_custom_widget_manager()

map_ = KeplerGl(height=700)
map_.add_data(
    data=overlay_df[overlay_df.area<1],
    name='overlay_df'
)
map_.add_data(
    data=district_border,
    name='dist_border'
)
map_

"""# 儀表板資料格式轉換

For the input format of the dashboard, you can refer to this [link](https://tuic.gov.taipei/documentation/front-end/chart-data).

## save as GeoJson
"""

dis_longterm.drop(['geometry'], axis=1, inplace=True)
dis_longterm.rename(columns={'buffer': 'geometry'}, inplace=True)
print(dis_longterm.head(10))
dis_longterm['fax'] = ''
dis_longterm['is_accessi'] = ''
dis_longterm['code'] = ['C01010001', 'C01010002', 'C01010003', 'C01010004']
print(dis_longterm.head(10))

dis_institution.head(3)

# change crs
dis_longterm = dis_longterm.to_crs('EPSG:4326')

print(dis_institution)
dis_institution['geometry'] = dis_institution['geometry'].buffer(1000)
dis_institution = dis_institution.to_crs('EPSG:4326')
print(dis_institution)

dis_friendly_institution = pd.concat([dis_institution, dis_longterm])
dis_friendly_institution

dis_friendly_institution.shape

dis_friendly_institution.to_file(
    'dis_origin_radius.geojson',
    driver='GeoJSON'
)

dis_friendly_institution.head(3)

dis_friendly_institution['geometry'] = dis_friendly_institution['geometry'].centroid
dis_friendly_institution.to_file(
    'dis_origin_center.geojson',
    driver='GeoJSON'
)

"""## save as json

### Two-Dimensional Data
"""

stat = dis_friendly_institution.groupby('town').size().to_dict()

specified_order = [
    '北投區', '士林區', '內湖區', '南港區',
    '松山區', '信義區', '中山區', '大同區',
    '中正區', '萬華區', '大安區', '文山區'
]

results = {
    'data': [
        {
            'name':'',
            'data': [
                {
                    'x': x,
                    'y': stat[x]
                }
                for x in specified_order
            ]
        }
    ]
}

results

with open('dis_friendly_institution.json', "w") as json_file:
    json.dump(results, json_file, ensure_ascii=False)



"""### Three-Dimensional Data"""

# get Processed data "無障礙需求歷年趨勢"
RAW_DATA_URL = 'https://raw.githubusercontent.com/tpe-doit/Taipei-Codefest-2023-Workshop/3-ETL/Datasets/Processed/%E7%84%A1%E9%9A%9C%E7%A4%99%E9%9C%80%E6%B1%82%E6%AD%B7%E5%B9%B4%E8%B6%A8%E5%8B%A2.csv?token=GHSAT0AAAAAACJI45ICIGH62EBWZCC7DYE4ZJ7OINQ'
FILE_NAME = '無障礙需求歷年趨勢.csv'
!wget $RAW_DATA_URL -O $FILE_NAME
disability_demand_trend = pd.read_csv(FILE_NAME, thousands=',')

disability_demand_trend['年份'] = disability_demand_trend['年份'].str.replace('年', '-01-01')
disability_demand_trend['年份'] = convert_str_to_time_format(
    column=disability_demand_trend['年份'],
    from_format='y-m-d'
)
disability_demand_trend['年份'] = disability_demand_trend['年份'].map(
    lambda x: x.isoformat()
).str.replace('\+08:00', 'Z')

disability_demand_trend.head(3)

cols = list(disability_demand_trend)
cols.remove('年份')

results = {
    'data': [
        {
            'name': col,
            'data': [
                {
                    'x': x,
                    'y': y
                }
                for x, y in zip(
                    disability_demand_trend['年份'],
                    disability_demand_trend[col]
                )
            ]
        }
        for col in cols
    ]
}

with open('disability_demand_trend.json', "w") as json_file:
    json.dump(results, json_file, ensure_ascii=False)

results

"""# 實作 - 整理儀表板可用的資料

**原始資料集**
身障就業資源地圖_1120918

URL:https://data.taipei/dataset/detail?id=c5aafda8-ef14-4f66-a6b7-d5da995a14b5

備註:原始資料集之地址欄位填寫狀況較多，如同一欄位有多筆地址或門牌號碼有多筆的狀況，建議可依照自身考慮條件進行清理與優化。
"""

import pandas as pd
import geopandas as gpd

# raw_data
RAW_DATA_URL = 'https://raw.githubusercontent.com/tpe-doit/Taipei-Codefest-2023-Workshop/3-ETL/Datasets/Raw/%E8%BA%AB%E9%9A%9C%E5%B0%B1%E6%A5%AD%E8%B3%87%E6%BA%90%E5%9C%B0%E5%9C%96_1120918.csv?token=GHSAT0AAAAAACIZES6SCQF26KZKWWSQHMMYZJ7QMVA'
FILE_NAME = 'Practice_accessibility_job_map.geojson'
!wget $RAW_DATA_URL -O $FILE_NAME
raw_data = pd.read_csv(FILE_NAME,encoding='big5')

raw_data.head()

"""**最終產出成果**

共有二個檔案，分為基本圖資(chart_data)與地圖圖資(map_data)


**地址清理上採取以下動作:**

(1)去除新北市的機構(2筆)

(2)同一欄位有多筆地址換行取代為空格

(3)若有基金會與辦公室地址，以基金會為主

(4)若存在多筆地址，暫取第一筆地址作為代表呈現(可依個人判斷優化操作)

(5)若geocoder無回傳經緯度欄位，暫先去除該筆資料(可依個人判斷優化操作)(1筆)
"""

# 最終產出成果如下
# map_data
RAW_DATA_URL = 'https://raw.githubusercontent.com/tpe-doit/Taipei-Codefest-2023-Workshop/3-ETL/Processed_by_ETL/map_data/Practice_accessibility_job_map.geojson?token=GHSAT0AAAAAACIZES6SZWU6RPEZEQFKQCC2ZJ7QMEA'
FILE_NAME = 'Practice_accessibility_job_map.geojson'
!wget $RAW_DATA_URL -O $FILE_NAME
Practice_accessibility_job_map = gpd.read_file(FILE_NAME,encoding='utf-8')

Practice_accessibility_job_map.head()

# chart_data
RAW_DATA_URL = 'https://raw.githubusercontent.com/tpe-doit/Taipei-Codefest-2023-Workshop/3-ETL/Processed_by_ETL/chart_data/Practice_accessibility_job_town.json?token=GHSAT0AAAAAACIZES6S6VEKSOWYHIB3YHR6ZJ7QNVQ'
FILE_NAME = 'Practice_accessibility_job_town.json'
!wget $RAW_DATA_URL -O $FILE_NAME
Practice_accessibility_job_town = pd.read_json(FILE_NAME,encoding='utf-8')

Practice_accessibility_job_town